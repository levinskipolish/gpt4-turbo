"""
GPT-4 Enhanced Chatbot Script

This Python script showcases an advanced chatbot leveraging OpenAI's GPT-4 language model through the `langchain` library. Designed to provide an immersive conversational experience, the chatbot comes with a range of features.

ðŸŒŸ Key Features:
- Harnesses the power of GPT-4 for seamless natural language understanding and generation.
- Implements a vibrant and visually appealing chat interface with colored terminal output.
- Manages and persists conversation history, storing it in a user-friendly JSON format (`conversation.json`).
- Facilitates an interactive conversation loop, engaging users with personalized prompts.

ðŸš€ How to Use:
1. Ensure dependencies are installed using pipenv.
2. Run the script to launch the GPT-4 chatbot.
3. Enter messages during the interactive conversation.
4. Type "exit" to gracefully conclude the conversation and save the chat history.

ðŸ“¦ Dependencies:
- langchain (Ensure it supports GPT-4)
- OpenAI GPT-4
- Python (version 3.x)

ðŸ‘¨â€ðŸ’» Author: Cleiton Levinski
"""
from langchain.memory import ConversationBufferMemory
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
from langchain.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from typing import Any, Dict, List
from langchain.schema import LLMResult
import json
import sys
from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser
from langchain.prompts import BaseChatPromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chat_models import ChatOpenAI
from typing import List, Union
from langchain.schema import AgentAction, AgentFinish, HumanMessage
import re
import os
from langchain.utilities.google_serper import GoogleSerperAPIWrapper

google_serper = GoogleSerperAPIWrapper(serper_api_key="your_actual_serpapi_key_here")

# Define which tools the agent can use to answer user queries
# GPT-4 model configuration
GPT_MODEL = "gpt-4-1106-preview"

class CustomStreamingHandler(StreamingStdOutCallbackHandler):
    """
    Custom callback handler for streaming GPT-4 output with colored formatting.
    """

    final_answer = ""  # Flag to track the beginning of the stream

    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        """
        Callback triggered for each new token generated by GPT-4.
        """
        self.final_answer += token
        
        # sys.stdout.write(f"\033[34m{data}")  # Colored output in bl
        cot_answer_pattern = re.compile(r'Thought:(.+)')
        final_answer_pattern = re.compile(r'Final Answer:(.+)')
        if(final_answer_pattern.search(self.final_answer)):
            sys.stdout.write(f"\033[34m{token}")  # Colored output in blue

        elif(cot_answer_pattern.search(self.final_answer)):    
            sys.stdout.write(f"\033[31m{token}")  # Colored output in red
        sys.stdout.flush()

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """
        Callback triggered when GPT-4 processing ends.
        """
        sys.stdout.write(f"\n")
        sys.stdout.flush()
        self.final_answer = ""

# Language Model (LLM) initialization
llm = ChatOpenAI(model=GPT_MODEL, streaming=True, callbacks=[CustomStreamingHandler()])

SERPAPI_API_KEY = os.environ['SERPER_API_KEY']
# Define which tools the agent can use to answer user queries
search = GoogleSerperAPIWrapper(serper_api_key=SERPAPI_API_KEY)

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events"
    )
]


template = """
    Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:
    
    {tools}

    Use the following format is necessary:

    Question: the input question you must answer
    Thought::[Ai] you should always think about what to do
    Action: the action to take, should be one of [{tool_names}]
    Action Input: the input to the action
    Observation: the result of the action
    ... (this Thought/Action/Action Input/Observation can repeat N times)
    Thought: I now know the final answer
    Final Answer::[Ai] the final answer to the original input question

    These were previous tasks you completed:

    Begin!
    
    Chat History: {chat_history}
    
    Question: {input}
    {agent_scratchpad}
"""
# Set up a prompt template
class CustomPromptTemplate(BaseChatPromptTemplate):
    # The template to use
    template: str
    # The list of tools available
    tools: List[Tool]

    def format_messages(self, **kwargs) -> str:
        # Get the intermediate steps (AgentAction, Observation tuples)
        # Format them in a particular way
        intermediate_steps = kwargs.pop("intermediate_steps")
        thoughts = ""
        for action, observation in intermediate_steps:
            thoughts += action.log
            thoughts += f"\nObservation: {observation}\nThought: "
        # Set the agent_scratchpad variable to that value
        kwargs["agent_scratchpad"] = thoughts
        # Create a tools variable from the list of tools provided
        kwargs["tools"] = "\n".join([f"{tool.name}: {tool.description}" for tool in self.tools])
        # Create a list of tool names for the tools provided
        kwargs["tool_names"] = ", ".join([tool.name for tool in self.tools])
        formatted = self.template.format(**kwargs)
        return [HumanMessage(content=formatted)]

class CustomOutputParser(AgentOutputParser):

    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:
        # Check if agent should finish
        if "Final Answer:" in llm_output:
            return AgentFinish(
                # Return values is generally always a dictionary with a single `output` key
                # It is not recommended to try anything else at the moment :)
                return_values={"output": llm_output.split("Final Answer:")[-1].strip()},
                log=llm_output,
            )
        # Parse out the action and action input
        regex = r"Action\s*\d*\s*:(.*?)\nAction\s*\d*\s*Input\s*\d*\s*:[\s]*(.*)"
        match = re.search(regex, llm_output, re.DOTALL)
        if not match:
            raise ValueError(f"Could not parse LLM output: `{llm_output}`")
        action = match.group(1).strip()
        action_input = match.group(2)
        # Return the action and action input
        return AgentAction(tool=action, tool_input=action_input.strip(" ").strip('"'), log=llm_output)
    
# Prompt configuration
prompt = CustomPromptTemplate(
    messages=[
        SystemMessagePromptTemplate.from_template(
            "You are a nice chatbot having a conversation with a human."
        ),
        # The `variable_name` here aligns with memory
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessagePromptTemplate.from_template("{question}"),
    ],
    template=template,
    tools=tools,
    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically
    # This includes the `intermediate_steps` variable because that is needed
    input_variables=["input", "intermediate_steps", "chat_history"]
)

output_parser = CustomOutputParser()

# Conversation memory initialization
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Load conversation history from file
conversation_file_path = "./conversation.json"

try:
    with open(conversation_file_path, "r") as file:
        file_content = file.read()
        messages = json.loads(file_content)

    # If messages exist, add them to the conversation memory
    if messages:
        for message in messages:
            dict_key = [key for key in message][0]
            dict_value = message[dict_key]
            if dict_key == 'user':
                memory.chat_memory.add_user_message(dict_value)
            else:
                memory.chat_memory.add_ai_message(dict_value)
except FileNotFoundError:
    print(f'The file "{conversation_file_path}" does not exist. A new one will be created.')
except Exception as e:
    print(e)

# Conversation chain initialization
conversation = LLMChain(llm=llm, prompt=prompt, verbose=False)

tool_names = [tool.name for tool in tools]

agent = LLMSingleActionAgent(
    llm_chain=conversation,
    output_parser=output_parser,
    stop=["\nObservation:"],
    allowed_tools=tool_names,
)

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False, memory=memory)

# Interactive conversation loop
conv = True
while conv:
    user_input = input("\n\033[32m::[You] ")

    if user_input == "exit":
        conv = False

    #conversation({"question": user_input})
    agent_executor.invoke(user_input)

# Extract chat history
chat_history = []

memory_chat = memory.chat_memory.dict()['messages']

for chat in memory_chat:
    chat_type = chat['type'] if chat['type'] == 'ai' else 'user'
    message = {chat_type: chat['content']}
    chat_history.append(message)

# Save conversation history to file
with open(conversation_file_path, "w") as file:
    file.write(json.dumps(chat_history))
